
*****

####  Synopses6087  
##### 1#       楼主       发表于 2024-6-7 18:06

我文化水平低，看不懂高科技新闻，也不知道是新闻学魅力时刻还是阿里真的做了个能上新闻的东西
[https://www.zhihu.com/question/6 ... 1782473043581329408](https://www.zhihu.com/question/658307301/answer/3523276880?utm_psn=1782473043581329408)

*****

####  万恶淫猥手  
##### 2#       发表于 2024-6-7 18:12

搜一下通义千问，那公关稿可不少的

*****

####  Gotu  
##### 3#       发表于 2024-6-7 19:01

这里能任选两个AI对比

[https://www.modelscope.cn/studio ... ummary?fullScreen=1](https://www.modelscope.cn/studios/opencompass/CompassArena/summary?fullScreen=1)

*****

####  蛋饼  
##### 4#       发表于 2024-6-7 19:07

虽然是商稿，但qwen确实做得还行，而且是国内硕果仅存的几个开源llm之一了

*****

####  Synopses6087  
##### 5#         楼主| 发表于 2024-6-7 19:11

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=65145872&amp;ptid=2186623" target="_blank">蛋饼 发表于 2024-6-7 19:07</a>

虽然是商稿，但qwen确实做得还行，而且是国内硕果仅存的几个开源llm之一了</blockquote>
这个 qwen 和应用市场里免费的通义千问是一个 app 吗

*****

####  泰坦失足  
##### 6#       发表于 2024-6-7 19:13

Qwen一直是开源LLM之光的，每次发布都是刷新当前开源LLM最佳成绩，YI做到YI-large说不开源了，Mixtral最近才放出8x22b。盘古模型到现在连Benchmark evaluation都没，就在那里看自媒体硬吹

*****

####  lukesweet  
##### 7#       发表于 2024-6-7 19:14

确实很不错，一些应用层实测甚至略强于GLM，阿里又是投资又是自己做开源，还都做得不错，真的猛，和朋友开玩笑说格局大得有点不像阿里

*****

####  7uly  
##### 8#       发表于 2024-6-7 19:15

qwen一直是开源大模型里第一梯队的 确实比较厉害

*****

####  蛋饼  
##### 9#       发表于 2024-6-7 19:36

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=65145922&amp;ptid=2186623" target="_blank">lukesweet 发表于 2024-6-7 19:14</a>

确实很不错，一些应用层实测甚至略强于GLM，阿里又是投资又是自己做开源，还都做得不错，真的猛，和朋友开 ...</blockquote>
毕竟modelscope也是阿里的，这个山寨hf能做起来价值大多了吧

*****

####  2474089352  
##### 10#       发表于 2024-6-7 19:40

qwen2开源协议改了，现在它真是国产开源之光了

[论坛助手,iPhone](https://bbs.saraba1st.com/2b/forum.php?mod=viewthread&amp;tid=2029836)

*****

####  naiveyan  
##### 11#       发表于 2024-6-7 19:44

阿里做得最好的一点是愿意反哺开源环境，国际上给流行的开源库都交了代码，墙内搞了魔搭和各种docker/pypi镜像，保证一发布全球都能用上（虽然qwen2在llama.cpp上还是翻车了），论使用方便仅次于llama。

单论国产开源权重模型，glm4，yi1.5也一直在出，智源的aquila虽然不出了但是bge还是在出，真闭源的也就baichuan，不至于“硕果仅存”

模型本身倒是中规中矩，体感文科类的功能（写作/翻译）上甚至比起上个版本最大的110b有退步，还得等超大杯

以及这次发布最有意思的点明明是画了个开源全模态模型，支持音像理解的饼，希望能比meta那边先出

*****

####  诚司  
##### 12#       发表于 2024-6-7 19:51

qwen2虽然榜上数据很强，但实际用感觉还是并没有llama3强

不过商不商稿的就没必要了，30b+尺寸的开源大模型都是全人类财富，阿里花钱做慈善还黑他何必呢

qwen2必然是最强开源中文大模型，也大概率是最强日语越南语韩语大模型，在业务级任务上逻辑推理能力提升很多了<img src="https://static.saraba1st.com/image/smiley/face2017/009.gif" referrerpolicy="no-referrer">

*****

####  ufo0000  
##### 13#       发表于 2024-6-7 20:01

写八股文，通义几乎秒杀市面其他

*****

####  诚司  
##### 14#       发表于 2024-6-7 20:09

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=65145908&amp;ptid=2186623" target="_blank">泰坦失足 发表于 2024-6-7 19:13</a>

Qwen一直是开源LLM之光的，每次发布都是刷新当前开源LLM最佳成绩，YI做到YI-large说不开源了，Mixtral最近 ...</blockquote>
盘古有n个版本，我问过华为的人，他们基本觉得最高也就gpt3.5水平，而且还是175b的这个水平……

不过华为作为卖显卡的，这都不重要……

*****

####  weiyang  
##### 15#       发表于 2024-6-7 20:11

之前参加过一个会，阿里专家就说会坚持开源，因为无论搞什么样的大模型，都需要算力，都需要服务器，只要能推广，阿里怎么都有得赚

*****

####  alixsander  
##### 16#       发表于 2024-6-7 21:12

当然是真的，Qwen 1.5就很热门

*****

####  alixsander  
##### 17#       发表于 2024-6-7 21:13

<blockquote>泰坦失足 发表于 2024-6-7 19:13
Qwen一直是开源LLM之光的，每次发布都是刷新当前开源LLM最佳成绩，YI做到YI-large说不开源了，Mixtral最近 ...</blockquote>
盘古就算了，向上管理骗骗可以，放出来一坨

*****

####  Risa  
##### 18#       发表于 2024-6-7 21:15

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=65146447&amp;ptid=2186623" target="_blank">weiyang 发表于 2024-6-7 20:11</a>

之前参加过一个会，阿里专家就说会坚持开源，因为无论搞什么样的大模型，都需要算力，都需要服务器，只要能 ...</blockquote>
所以这次不给32B和14B了，

32B单卡24G用，14B16G卡用，用爽了就不买服务器了。

*****

####  Van夫膜开  
##### 19#       发表于 2024-6-7 21:18

阿里这个模型确实很强之前的codeqwen 1.5 用7b的规模性能比deepseekcoder 33b还强，在evalplus排行榜上超越了gpt3.5。

今天好几个群里面反映对于复杂指令，qwen2 72b的性能居然比gpt4还强。

与其他团队不同，qwen团队不仅发布了模型，还一带把awq，gptq还有gguf文件都放出来了，真的很良心了。

*****

####  塔奇克马  
##### 20#       发表于 2024-6-7 21:19

什么时候有72b q2 量化啊

—— 来自 [鹅球](https://www.pgyer.com/xfPejhuq) v3.0.0.82-alpha

*****

####  Van夫膜开  
##### 21#       发表于 2024-6-7 21:25

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=65147091&amp;ptid=2186623" target="_blank">塔奇克马 发表于 2024-6-7 21:19</a>

什么时候有72b q2 量化啊

—— 来自 鹅球 v3.0.0.82-alpha</blockquote>
q2量化的准确率有点不忍直视了

*****

####  塔奇克马  
##### 22#       发表于 2024-6-7 21:28

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=65147152&amp;ptid=2186623" target="_blank">Van夫膜开 发表于 2024-6-7 21:25</a>

q2量化的准确率有点不忍直视了</blockquote>
<img src="https://static.saraba1st.com/image/smiley/face2017/125.png" referrerpolicy="no-referrer">和7B同样显存比咋样？

*****

####  ycjiang1337  
##### 23#       发表于 2024-6-7 21:38

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=65146428&amp;ptid=2186623" target="_blank">诚司 发表于 2024-6-7 20:09</a>

盘古有n个版本，我问过华为的人，他们基本觉得最高也就gpt3.5水平，而且还是175b的这个水平……

不过华为 ...</blockquote>
你看NV自己做过什么牛逼的大模型么？华为现在卖卡卖的飞起，数钱数到手抽筋，包括我司在内的国内头部互联网大厂基本都买了万卡昇腾集群，他真有那闲工夫不如好好把多机多卡通信再调好一点

*****

####  Risa  
##### 24#       发表于 2024-6-7 21:39

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=65147152&amp;ptid=2186623" target="_blank">Van夫膜开 发表于 2024-6-7 21:25</a>

q2量化的准确率有点不忍直视了</blockquote>
不会吧，模型参数规模越大，量化损失越小，

之前32B Q2都没啥太大损失，14B Q3以内也不会太大损失，

72B Q2应该性能不会达到疑惑率大幅升高的拐点后。

*****

####  ycjiang1337  
##### 25#       发表于 2024-6-7 21:39

Qwen1.5曾经是全球最强开源大模型，一直到LLama3出来才被超越。前两天Qwen2还没发布，推上就有很多白皮敲碗等更新了。

*****

####  ycjiang1337  
##### 26#       发表于 2024-6-7 21:41

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=65146146&amp;ptid=2186623" target="_blank">naiveyan 发表于 2024-6-7 19:44</a>

阿里做得最好的一点是愿意反哺开源环境，国际上给流行的开源库都交了代码，墙内搞了魔搭和各种docker/pypi ...</blockquote>
其实论使用方便已经超过LLama了，毕竟不用申请。之前LLama3首发的时候有些在硅谷的中国人甚至是从Modelscope上下载的LLama3权重，因为不用等审批

*****

####  omnitoken  
##### 27#       发表于 2024-6-7 21:45

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=65145908&amp;ptid=2186623" target="_blank">泰坦失足 发表于 2024-6-7 19:13</a>

Qwen一直是开源LLM之光的，每次发布都是刷新当前开源LLM最佳成绩，YI做到YI-large说不开源了，Mixtral最近 ...</blockquote>
emmm 盘古基本没有对C的应用的, 实际上气象预测和政府相关项目才是大头

*****

####  treexper  
##### 28#       发表于 2024-6-7 21:49

llm到底看哪个榜，不是这个么？
[https://chat.lmsys.org/?leaderboard](https://chat.lmsys.org/?leaderboard)

*****

####  ycjiang1337  
##### 29#       发表于 2024-6-7 22:07

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=65147366&amp;ptid=2186623" target="_blank">treexper 发表于 2024-6-7 21:49</a>

llm到底看哪个榜，不是这个么？

https://chat.lmsys.org/?leaderboard</blockquote>
这是PVP榜，也算是其中一个

*****

####  Nanachi  
##### 30#       发表于 2024-6-8 00:43

价格也挺贵的，40元/百万

*****

####  fmketchup  
##### 31#       发表于 2024-6-8 00:52

mmp剩下就看老黄狗给不给顶级游戏卡加显存了。

*****

####  Ameyoru  
##### 32#       发表于 2024-6-8 01:12

在推上搜，居然看到了这个<img src="https://static.saraba1st.com/image/smiley/face2017/067.png" referrerpolicy="no-referrer">

<img src="https://img.saraba1st.com/forum/202406/08/011232m5zwfzgzzgg1ipt6.jpeg" referrerpolicy="no-referrer">" src="https://static.saraba1st.com/image/common/none.gif" referrerpolicy="no-referrer">

<strong>IMG_0343.jpeg</strong> (807.12 KB, 下载次数: 0)

下载附件

2024-6-8 01:12 上传

*****

####  ycjiang1337  
##### 33#       发表于 2024-6-8 01:21

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=65149994&amp;ptid=2186623" target="_blank">Ameyoru 发表于 2024-6-8 01:12</a>

在推上搜，居然看到了这个</blockquote>
笑死，目前同级别支持日语的开源大模型完全没有，LLama3的多语言基本上是废的。有种给日本人喂屎的快感

*****

####  afer  
##### 34#       发表于 2024-6-8 01:23

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=65149994&amp;ptid=2186623" target="_blank">Ameyoru 发表于 2024-6-8 01:12</a>

在推上搜，居然看到了这个</blockquote>
好！支持，无比强大！

*****

####  mimighost  
##### 35#       发表于 2024-6-8 01:42

看那个pvp榜吧，mmlu什么的我已经不看了，没意义，mistral之前说什么和gpt4一个水平，一用就露馅

甚至gpt4-o我也觉得不如之前gpt4的老版本，昨天一个简单的python函数就是写不对

*****

####  教练  
##### 36#       发表于 2024-6-8 01:47

阿里的模型是拿老黄的卡，还是菊花的卡训练的？老黄的话，短中期会不会遇到瓶颈（无法购买最新的卡，等等）

*****

####  ycjiang1337  
##### 37#       发表于 2024-6-8 02:00

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=65150228&amp;ptid=2186623" target="_blank">mimighost 发表于 2024-6-8 01:42</a>

看那个pvp榜吧，mmlu什么的我已经不看了，没意义，mistral之前说什么和gpt4一个水平，一用就露馅

甚至gpt4 ...</blockquote>
PVP榜比LLama3-70B低一点，考虑到语言可以认为起码是平手。另外GPT-4o确实比GPT-4有退化

*****

####  ycjiang1337  
##### 38#       发表于 2024-6-8 02:00

 本帖最后由 ycjiang1337 于 2024-6-8 02:02 编辑 
<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=65150264&amp;ptid=2186623" target="_blank">教练 发表于 2024-6-8 01:47</a>

阿里的模型是拿老黄的卡，还是菊花的卡训练的？老黄的话，短中期会不会遇到瓶颈（无法购买最新的卡，等等） ...</blockquote>
国内几家大厂都采购了万卡910B集群，多机多卡互联彻底调顺了之后基本能等价于万卡A100集群。

然后世界上最强大的大模型是GPT-4，在它训练的时候全世界最先进的卡就是A100，不存在更先进的——这是假设之后没有910C或者920的情况下。

*****

####  s1234y  
##### 39#       发表于 2024-6-8 02:08

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=65150351&amp;ptid=2186623" target="_blank">ycjiang1337 发表于 2024-6-8 02:00</a>
 国内几家大厂都采购了万卡910B集群，多机多卡互联彻底调顺了之后基本能等价于万卡A100集群。  然后世界上 ...</blockquote>
910b连qwen1.5的推理都没跑顺，mindie这玩意用起来就是一坨翔，还有精度丢失问题，更别说训练了。互联网都是各种渠道买或者租n卡，目前其实不缺卡。

*****

####  ycjiang1337  
##### 40#       发表于 2024-6-8 02:16

 本帖最后由 ycjiang1337 于 2024-6-8 02:27 编辑 
<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=65150394&amp;ptid=2186623" target="_blank">s1234y 发表于 2024-6-8 02:08</a>

910b连qwen1.5的推理都没跑顺，mindie这玩意用起来就是一坨翔，还有精度丢失问题，更别说训练了。互联网 ...</blockquote>
你正好说反了，互联网大厂有合规要求反而不好买N卡。LLM现在是训练比推理简单，因为基于Paged Attention的超大吞吐量推理需要精细化调优——我司从Q2开始算法中台离线集群所有新上的资源组全都是910B，N卡资源组一个都没有。目前我们是用910B训大模型，用4090跑推理（后续可能升级到L20）。

起码对于Transformer类负载，目前我自己负责的模型负载已经全面迁到910B了，直接用torch2.1加上torch-npu，不存在任何精度和收敛性问题，单卡训练也不存在效率和兼容问题，实际速度（BERT全参和大模型LoRA微调）已经打平甚至超过A100了。然后我们部门自己用的大模型也已经在910B机器上完成了第一阶段持续预训练，同样不存在任何精度问题——哪怕是去年算法中台做验证的时候大模型相关的收敛性测试也都是一次通过的。目前我们用的算法中台提供的训练框架，只存在两个问题，一个是互联效率，另一个是多机模型并行的支持需要中台那边升级框架。当前过渡期我们正在把零散的中等负载都迁到910B上，把A100和H800腾出来集中使用。

<img src="https://img.saraba1st.com/forum/202406/08/022745bqq1x4wbra440ykl.png" referrerpolicy="no-referrer">

<strong>截屏2024-06-08 02.25.07.png</strong> (66.52 KB, 下载次数: 0)

下载附件

2024-6-8 02:27 上传


*****

####  ycjiang1337  
##### 41#       发表于 2024-6-8 02:26

 本帖最后由 ycjiang1337 于 2024-6-8 02:28 编辑 

风怒编辑

*****

####  s1234y  
##### 42#       发表于 2024-6-8 02:28

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=65150440&amp;ptid=2186623" target="_blank">ycjiang1337 发表于 2024-6-8 02:16</a>
 你正好说反了，互联网大厂有合规要求反而不好买N卡。LLM现在是训练比推理简单，因为基于Paged Attention的 ...</blockquote>
我了解互联网公司里边就美团买了910b，目前没见到他们训出来啥东西，有靠谱模型的公司里边智谱和讯飞和910b适配的还行。qwen1.5如果不用华为内部mindie rc2根本拉不起来推理，就算拉起来我手里还一大堆异常prompt华为没解决。ascend device plugin也有大坑。我手里就有推理精度下降的石锤证据，华为都认你不认<img src="https://static.saraba1st.com/image/smiley/face2017/009.gif" referrerpolicy="no-referrer">

整体看下来ascend工具链问题很多，还得再发现2-3年。

*****

####  ycjiang1337  
##### 43#       发表于 2024-6-8 02:29

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=65150521&amp;ptid=2186623" target="_blank">s1234y 发表于 2024-6-8 02:28</a>

我了解互联网公司里边就美团买了910b，目前没见到他们训出来啥东西，有靠谱模型的公司里边智谱和讯飞和91 ...</blockquote>
那说明你消息过时了，去年10月华为来找我们宣讲的时候提到的标杆客户就有一个美团一个腾讯，而且用的还是CTR任务。

然后我司不是美团


*****

####  mimighost  
##### 44#       发表于 2024-6-8 02:41

单卡和集群不是一个东西，集群跑飞的情况可太多了

甚至用a100跑的通的，a800就跑不通，这两个的计算核心应该是完全一样的，这两天知乎就有帖子聊这个东西

*****

####  ycjiang1337  
##### 45#       发表于 2024-6-8 02:45

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=65150596&amp;ptid=2186623" target="_blank">mimighost 发表于 2024-6-8 02:41</a>

单卡和集群不是一个东西，集群跑飞的情况可太多了

甚至用a100跑的通的，a800就跑不通，这两个的计算核心应 ...</blockquote>
然而我们同样也跑通了16卡训练，训出来的模型已经投入使用了——所以你想说什么？

多机多卡目前张量并行确实存在问题，但这也是在排期之内的。另外大部分业务需要的模型都可以通过一个一个的8-16卡分组解决，这一来一回就能放出来很多A100，极端情况下把所有A100集中起来使用都足够解决很多问题了。


*****

####  诚司  
##### 46#       发表于 2024-6-8 02:48

美团、讯飞、京东、百度至少是都用了昇腾910b的

不过qwen，当时华为的人问了我一个奇怪的问题，为什么要用qwen？我说除了qwen（那时候还没command R+）还有几个开源的参数多的中英文都支持好的大模型？大概确实没适配吧<img src="https://static.saraba1st.com/image/smiley/face2017/067.png" referrerpolicy="no-referrer">

*****

####  ycjiang1337  
##### 47#       发表于 2024-6-8 02:51

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=65150625&amp;ptid=2186623" target="_blank">诚司 发表于 2024-6-8 02:48</a>

美团、讯飞、京东、百度至少是都用了昇腾910b的

不过qwen，当时华为的人问了我一个奇怪的问题，为什么要用q ...</blockquote>
千问确实非常常用，基本上现在业务上用的开源大模型，差不多都是千问一统天下了，尤其电商领域。之前我们打比赛，KDD Cup第一阶段的测试集（英文电商问题）用千问1.5-7B性能碾压所有同尺寸模型，包括LLama3-8B。

目前实际结果跟预期完全相反，现在我们是910B训练，然后用N卡部署……部署的卡现在用的是4090，之后估计L20到了之后要换过去。


*****

####  诚司  
##### 48#       发表于 2024-6-8 02:53

刷了一下Chatbot Arena，发现qwen2低于预期的原因是中英之外的语言部分没上榜，感觉是Chatbot Arena对战里选模型的问题，因为qwen1.5排名都在那里，甚至llama2都在那里……

中英文都比command R+强，最后因为多语言没上榜导致在command R+后面……

cmd R+的多语言问题其实还挺大的，用中文提问回答能力比英文弱多了。这很正常，不过qwen从一开始就是中英双母语水平的，虽然有英文prompt蹦出来random Chinese token的问题，不过能力还是中英差不多（虽然可能是中英都烂），cmd R+就可能是英文烂中文更烂……


*****

####  ycjiang1337  
##### 49#       发表于 2024-6-8 02:57

 本帖最后由 ycjiang1337 于 2024-6-8 02:59 编辑 
<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=65150640&amp;ptid=2186623" target="_blank">诚司 发表于 2024-6-8 02:53</a>

刷了一下Chatbot Arena，发现qwen2低于预期的原因是中英之外的语言部分没上榜，感觉是Chatbot Arena对战里 ...</blockquote>
我感觉qwen的跨语言理解应该不是问题，KDD Cup那个英文电商测试集用qwen1.5-7B直接Zero-shot提交就能大比分吊打大多数其它模型……连LLama3-8B都被吊打了。

当时我们参赛的时候优化的方向甚至干脆就是如何在有限空间和时间限制里塞进去更大的qwen1.5……


*****

####  ycjiang1337  
##### 50#       发表于 2024-6-8 03:03

 本帖最后由 ycjiang1337 于 2024-6-8 03:05 编辑 

另外说到推理，最近算法中台在向我们推销他们的框架，号称910B能实现相当于A800-vllm方案的1.5倍性能。从他们的描述来看那个框架应该是基于MindIE的，但是里面的算子是他们自己写的，没有用华为的。目前我们部门暂时没有这么大的推理需求，所以还没接触。


*****

####  诚司  
##### 51#       发表于 2024-6-8 03:09

 本帖最后由 诚司 于 2024-6-8 03:11 编辑 
<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=65146146&amp;ptid=2186623" target="_blank">naiveyan 发表于 2024-6-7 19:44</a>

阿里做得最好的一点是愿意反哺开源环境，国际上给流行的开源库都交了代码，墙内搞了魔搭和各种docker/pypi ...</blockquote>
qwen1时代就有qwen VL和qwen audio，估计也就是整合重新练一下，audio还好，VL按现在的开放程度，不会放太好的模型出来，qwen VL max和开源版的水平就是天差地别……

更期待的还是gpt-o那种低时延的支持audio模态实时输出的模型，现在我用的tts，要么时延大要么语气生硬，毕竟vits模型是要把text全输入才能输出的……gpt-o这种活感觉不难做但是还是适合训大模型的人做

明明低时延聊天应该是一看就商业价值满满的路线，不知道为什么没人做……


*****

####  s1234y  
##### 52#       发表于 2024-6-8 03:24

<blockquote><a href="httphttps://bbs.saraba1st.com/2b/forum.php?mod=redirect&amp;goto=findpost&amp;pid=65150528&amp;ptid=2186623" target="_blank">ycjiang1337 发表于 2024-6-8 02:29</a>
 首先你消息完全不灵通，去年10月华为来宣讲的时候提到的标杆互联网客户就至少有美团腾讯百度这仨，百度用9 ...</blockquote>
我们用客户的910b跑了基于qwen-14b的nl2sql子任务全参sft，训完跑测试集，和A100同样数据全参sft训出来的模型对比，sql正确率差了1.3%。我们冒充客户去和华为的人沟通，华为开始给的解释是说可能是权重转换完就有差异，后来发现我们不是客户自己人干脆不理我们了。
推理这么一个基本的事情现在都跑的稀碎，出任何小问题就让客户从驱动开始升级工具链全家桶，升级的版本还都是小版本号很接近的rc版本，这些都给我一种把客户当小白鼠的印象。
说实话有没有page attention这些优化我觉得都无所谓，就正常把模型推理跑起来，批量跑测试prompt不要有上百个乱码的case，把国内开源的hf格式的模型做好兼容，ascend做好这些真的就可以了。

